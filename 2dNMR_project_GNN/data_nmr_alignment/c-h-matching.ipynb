{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/yunruili/anaconda_env/diffusion/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/work/yunruili/2dNMR_project_GNN/')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from load_alignment_graph_nmr import graph_nmr_data, custom_collate_fn\n",
    "from Comenet_NMR import ComENet\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load multi task model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/work/yunruili/2dNMR_project_GNN/')\n",
    "from load_graph_cnmr_hnmr_alignment import graph_nmr_alignment_data, custom_collate_fn, CustomBatchSampler\n",
    "from Comenet_NMR_multitask import ComENet\n",
    "\n",
    "graph_path = '/scratch0/haox/2DNMR_prediction_gt/Datasets/graph3d/'\n",
    "nmr_path = '/scratch0/haox/yunruili/'\n",
    "# cnmr_path = '/scratch0/haox/yunruili/cnmr_alignment'\n",
    "# hnmr_path = '/scratch0/haox/yunruili/hnmr_alignment'\n",
    "csv_cnmr = '../filtered_cnmr_smile_dataset_22k.csv'\n",
    "csv_hnmr = '../filtered_hnmr_smile_dataset_67.csv'\n",
    "csv_common = '../filtered_common_smile_dataset_1600.csv'\n",
    "dataset_c = graph_nmr_alignment_data(csv_cnmr, graph_path, nmr_path, type='c')\n",
    "dataset_h = graph_nmr_alignment_data(csv_hnmr, graph_path, nmr_path, type='h')\n",
    "dataset_ch = graph_nmr_alignment_data(csv_common, graph_path, nmr_path, type='both')\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(0)  # Replace your_seed with your chosen seed value\n",
    "\n",
    "# Define the proportions or absolute sizes for your train and val sets\n",
    "train_size_c = int(0.8 * len(dataset_c))\n",
    "val_size_c = len(dataset_c) - train_size_c\n",
    "\n",
    "train_size_h = int(0.8 * len(dataset_h))\n",
    "val_size_h = len(dataset_h) - train_size_h\n",
    "\n",
    "train_size_ch = int(0.8 * len(dataset_ch))\n",
    "val_size_ch = len(dataset_ch) - train_size_ch\n",
    "\n",
    "# Split the datasets\n",
    "train_dataset_c, val_dataset_c = random_split(dataset_c, [train_size_c, val_size_c])\n",
    "train_dataset_h, val_dataset_h = random_split(dataset_h, [train_size_h, val_size_h])\n",
    "train_dataset_ch, val_dataset_ch = random_split(dataset_ch, [train_size_ch, val_size_ch])\n",
    "\n",
    "# Create DataLoaders for training and validation sets\n",
    "train_dataloader_c = DataLoader(train_dataset_c, batch_size=1, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_dataloader_c = DataLoader(val_dataset_c, batch_size=1, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "train_dataloader_h = DataLoader(train_dataset_h, batch_size=1, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_dataloader_h = DataLoader(val_dataset_h, batch_size=1, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "train_dataloader_ch = DataLoader(train_dataset_ch, batch_size=1, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_dataloader_ch = DataLoader(val_dataset_ch, batch_size=1, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Now you can create custom loaders for each set\n",
    "train_custom_loader = CustomBatchSampler(train_dataloader_c, train_dataloader_ch, train_dataloader_h, n1=7, n2=80)\n",
    "val_custom_loader = CustomBatchSampler(val_dataloader_c, val_dataloader_ch, val_dataloader_h, n1=14, n2=141)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "hc = 256\n",
    "chc=[128]\n",
    "hhc=[128, 64, 64]\n",
    "n = 3\n",
    "n_out = 2\n",
    "model = ComENet(in_embed_size=3, out_channels=1, agg_method='sum',\\\n",
    "     hidden_channels=hc, c_out_hidden=chc, h_out_hidden=hhc, num_layers=n, num_output_layers=n_out)\n",
    "msg = model.load_state_dict(torch.load(\n",
    "    '../gnn3d_multi_align_sum_hiddendim_%d_nlayers_%d_noutlayers_%d_couthidden_%s_houthidden_%s.pt'\\\n",
    "    %(hc, n, n_out, ''.join(str(i) for i in chc), ''.join(str(i) for i in hhc)), map_location='cpu'))\n",
    "print(msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apply on 2dnmr data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on 2d NMR data\n",
    "from torch_geometric.data import DataLoader as loader_2dnmr\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "class graph_nmr_data_2d(Dataset):\n",
    "    '''Returns the index of non-zero values on y-axis and the corresponding x-axis'''\n",
    "    def __init__(self, csv_file, graph_path, nmr_path, x='C'):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        self.file_list = df['File_name'].to_list()\n",
    "        # filter out Test_*.csv\n",
    "        self.file_list = [x for x in self.file_list if not x.startswith('Test_')]\n",
    "\n",
    "        self.nmr_path = nmr_path\n",
    "        self.graph_path = graph_path\n",
    "        self.x = x\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    def __getitem__(self, item):\n",
    "        filename = self.file_list[item].split('.')[0]\n",
    "\n",
    "        graph_file = os.path.join(self.graph_path, filename + '.pickle')\n",
    "        graph_data = pickle.load(open(graph_file, 'rb'))\n",
    "        graph_data.x = graph_data.x.float()\n",
    "        \n",
    "        graph_data.has_c = True\n",
    "        graph_data.has_h = True\n",
    "\n",
    "        # use original csv files so that the C nodes are not duplicated\n",
    "        nmr_file = os.path.join(self.nmr_path, filename + '.csv')\n",
    "        nmr_data = pd.read_csv(nmr_file)\n",
    "\n",
    "        # Forward fill the 'No.' column to fill empty values with the previous row's value\n",
    "        if 'No.' in nmr_data.columns:\n",
    "            nmr_data['No.'].fillna(method='ffill', inplace=True)\n",
    "            # Group by both 'No.' and '13C' columns and create a list of '1H' values\n",
    "            grouped = nmr_data.groupby(['No.', '13C'])['1H'].apply(list).reset_index()\n",
    "        else:\n",
    "            grouped = nmr_data.groupby('13C')['1H'].apply(list).reset_index()\n",
    "        c_list = grouped['13C'].tolist()\n",
    "        h_list = grouped['1H'].apply(lambda x: x if len(x) > 1 else [x[0], x[0]]).tolist()\n",
    "        c_peaks = torch.tensor(c_list).float().squeeze()/200\n",
    "        try:\n",
    "            h_peaks = torch.tensor(h_list).float()/10\n",
    "        except Exception as e:\n",
    "            print(f\"Error while converting to tensor for filename: {filename}\")\n",
    "            print(e)\n",
    "            h_peaks = torch.tensor([])\n",
    "\n",
    "\n",
    "        # this deals with the 1d mapped out file\n",
    "        # if self.x == 'C':\n",
    "        #     nonzero_nmr = nmr_data[nmr_data['1H']!=0]\n",
    "        # else:\n",
    "        #     nonzero_nmr = nmr_data[nmr_data['13C']!=0]\n",
    "        # c_peaks = torch.tensor(nonzero_nmr['13C'].tolist())\n",
    "        # h_peaks = torch.tensor(nonzero_nmr['1H'].tolist())\n",
    "\n",
    "        return graph_data, c_peaks, h_peaks, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/yunruili/anaconda_env/diffusion/lib/python3.8/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('33291',)\n",
      "5 5\n",
      "5 5\n"
     ]
    }
   ],
   "source": [
    "graph_path_2dnmr = '/scratch0/yunruili/2dnmr_30k/graph_3d/'\n",
    "csv_file_2dnmr = '../nmr_smile_solvent_filtered2_3dgnn.csv'\n",
    "nmr_path_2dnmr = '/scratch0/yunruili/2dnmr_30k/csv_30k/'\n",
    "save_dir = '/scratch0/yunruili/2dnmr_30k/nmr_2dcsv_chmatched'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "dataset_2dnmr = graph_nmr_data_2d(csv_file_2dnmr, graph_path_2dnmr, nmr_path_2dnmr)\n",
    "dataloader_2dnmr = loader_2dnmr(dataset_2dnmr, shuffle=True, batch_size=1)\n",
    "\n",
    "list_files = []\n",
    "for i, data in enumerate(dataloader_2dnmr):\n",
    "    graph, cnmr, hnmr, filename = data\n",
    "    \n",
    "    c_nodes = (graph.x[:,0]==5).nonzero(as_tuple=True)[0]\n",
    "    h_nodes = (graph.x[:, 0] == 0).nonzero(as_tuple=True)[0] \n",
    "\n",
    "    c_shifts, h_shifts = model(graph)\n",
    "\n",
    "    ##### calculate the indices of C node connected to H\n",
    "    # Initialize a list to store C nodes connected to H\n",
    "    c_nodes_connected_to_h = []\n",
    "    # Check each C node for connection to any H node\n",
    "    for c_node in c_nodes:\n",
    "        # Get indices of edges involving the C node\n",
    "        edges_of_c = (graph.edge_index[0] == c_node) | (graph.edge_index[1] == c_node)\n",
    "\n",
    "        # Get all nodes that are connected to this C node\n",
    "        connected_nodes = torch.cat((graph.edge_index[0][edges_of_c], graph.edge_index[1][edges_of_c])).unique()\n",
    "\n",
    "        # Check if any of these connected nodes are H nodes\n",
    "        if any(node in h_nodes for node in connected_nodes):\n",
    "            c_nodes_connected_to_h.append(c_node.item())\n",
    "    \n",
    "    # Convert to a tensor\n",
    "    c_nodes_connected_to_h = torch.tensor(c_nodes_connected_to_h)\n",
    "\n",
    "    c_shifts = c_shifts.flatten().detach().numpy()\n",
    "    h_shifts = h_shifts.flatten().detach().numpy()\n",
    "    cnmr = cnmr.squeeze().detach().numpy()\n",
    "    hnmr = hnmr.squeeze().detach().numpy()\n",
    "\n",
    "    c_index = [i for i, x in enumerate(c_nodes) if x in c_nodes_connected_to_h]\n",
    "    c_shifts = c_shifts[c_index]\n",
    "\n",
    "    print(filename)\n",
    "    print(len(c_shifts), len(h_shifts))\n",
    "    print(len(cnmr), len(hnmr))\n",
    "\n",
    "    # make c-h pairs for ground truth and prediction\n",
    "    hnmr = np.mean(hnmr, axis=1)\n",
    "    pred = np.stack([c_shifts, h_shifts], axis=1)\n",
    "    gt = np.stack([cnmr, hnmr], axis=1)\n",
    "\n",
    "    if i ==0:\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hungarian algorithm, for same length \n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial.distance import cdist\n",
    "from matching_code.ARG import ARG\n",
    "\n",
    "def match_samelen(pred, gt):\n",
    "    # Calculate the cost (distance) matrix\n",
    "    cost_matrix = cdist(pred, gt, metric='euclidean')\n",
    "\n",
    "    # Solve the assignment problem\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    # Output the matched pairs\n",
    "    matched_pairs = np.array(list(zip(row_ind, col_ind)))\n",
    "    return matched_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "## graduate assignment algorithm, for different length\n",
    "def compatibility(atr1, atr2):\n",
    "    #Consider the order to return 0 or inf\n",
    "    \n",
    "    if np.isnan(atr1).any() or np.isnan(atr1).any():\n",
    "        return 0\n",
    "    if (atr1 == float('Inf')).any() or (atr2 == float('Inf')).any():\n",
    "        return float('Inf')\n",
    "    if len(atr1) != len(atr2):\n",
    "        return 0\n",
    "    if (atr1 == 0).all() or (atr2 == 0).all():\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "    dim = len(atr1)\n",
    "    score = 1-((atr1-atr2)**2).sum()\n",
    "    #score = atr1 * atr2\n",
    "    return score\n",
    "\n",
    "def pre_compute_compatibility(ARG1, ARG2, alpha=1, stochastic=0, node_binary=True, edge_binary=True, dist_mask=None):\n",
    "    '''\n",
    "    Compute the best matching with two ARGs.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Size of the real match-in matrix\n",
    "    A = ARG1.num_nodes\n",
    "    I = ARG2.num_nodes\n",
    "    real_size = [A, I] # ???\n",
    "    augment_size = [A+1, I+1] # size of the matrix with slacks\n",
    "    \n",
    "    #### compute c_aibj ####\n",
    "    # nil node compatibility percentage\n",
    "    prct = 10\n",
    "    \n",
    "    ## pre-calculate the node compatibility\n",
    "    C_n = np.zeros(augment_size)\n",
    "    \n",
    "    if node_binary:\n",
    "        C_n[:A,:I] = cdist(ARG1.nodes_vector, ARG2.nodes_vector, compatibility_binary)\n",
    "    else:\n",
    "        C_n[:A,:I] = cdist(ARG1.nodes_vector, ARG2.nodes_vector, compatibility)\n",
    "    \n",
    "    # Add score to slacks\n",
    "    C_n[A,:-1] =  np.percentile(C_n[:A,:I],prct,0)\n",
    "    C_n[:-1,I] =  np.percentile(C_n[:A,:I],prct,1)\n",
    "    C_n[A,I] = 0 \n",
    "\n",
    "    # times the alpha weight\n",
    "    C_n = alpha*C_n\n",
    "    \n",
    "#     print(C_n)\n",
    "    \n",
    "    if dist_mask is not None:\n",
    "        C_n[:A,:I] = np.multiply(C_n[:A,:I], dist_mask)\n",
    "    \n",
    "    return C_n\n",
    "\n",
    "def graph_matching(C_n, ARG1, ARG2, beta_0=0.1, beta_f=20, beta_r=1.025, \n",
    "                   I_0=20, I_1=200, e_B=0.1, e_C=0.01, fixed_match = None):  # C_e, \n",
    "    ### fixed match is a matrix (A*I) for the pre-determined matching pairs \n",
    "    ##  We first do not consider the stochastic.\n",
    "    # set up the soft assignment matrix\n",
    "    \n",
    "    A = C_n.shape[0] - 1\n",
    "    I = C_n.shape[1] - 1 \n",
    "    m_Head = np.random.rand(A+1, I+1) # Not an assignment matrix. (Normalized??)\n",
    "    m_Head[-1,-1] = 0\n",
    "    \n",
    "    ### zero the nodes that already matched \n",
    "    if fixed_match is not None:\n",
    "        print('fixed some points')\n",
    "        C_n = np.multiply(C_n, fixed_match)\n",
    "\n",
    "    # Initialization for parameters\n",
    "\n",
    "    ## beta is the penalty parameters\n",
    "    # includes beta_0, beta_f, beta_r\n",
    "\n",
    "    ## I controls the maximum iteration for each round\n",
    "    # includes I_0 and I_1\n",
    "\n",
    "    ## e controls the range\n",
    "    # includes e_B and e_C\n",
    "\n",
    "    # begin matching\n",
    "    beta = beta_0\n",
    "\n",
    "    stochastic = False ### we first do not consider this case\n",
    "    \n",
    "    # the indexes of the non-zero elements in C_n\n",
    "    idx1 = np.unique(C_n.nonzero()[0])\n",
    "    idx2 = np.unique(C_n.nonzero()[1])  # not used, only check number\n",
    "#     print(A, len(idx1))\n",
    "#     print(I, len(idx2))\n",
    "    \n",
    "#     tmp_edges_1 = np.full([(A+1),(A+1)], np.nan)\n",
    "#     tmp_edges_1[:A,:A] = ARG1.edges_matrix\n",
    "#     tmp_edges_1[A,A] = float('Inf')\n",
    "    \n",
    "#     tmp_edges_2 = np.full([(I+1),(I+1)], np.nan)\n",
    "#     tmp_edges_2[:I,:I] = ARG2.edges_matrix\n",
    "#     tmp_edges_2[I,I] = float('Inf')\n",
    "\n",
    "    while beta < beta_f:\n",
    "\n",
    "        ## Round-B\n",
    "        #check if converges\n",
    "        converge_B = False\n",
    "        I_B = 0\n",
    "        while (not converge_B) and I_B <= I_0: # Do B until B is converge or iteration exceeds\n",
    "            if stochastic:\n",
    "                m_Head = m_Head ### + ???           \n",
    "\n",
    "#             print('I_B', m_Head[0])\n",
    "            old_B = m_Head # the old matrix\n",
    "            I_B += 1 \n",
    "\n",
    "            # Build the partial derivative matrix Q\n",
    "            Q = np.zeros([A+1, I+1])\n",
    "            \n",
    "            #### only calculate Q for the indexes positions\n",
    "\n",
    "            # Edge attribute\n",
    "#             for a in idx1:\n",
    "#                 ## get the non-zero (potential matches) in the second graph\n",
    "#                 idx2 = np.unique(np.nonzero(C_n[a]))\n",
    "#                 for i in idx2:\n",
    "#                     c_e = np.zeros([A+1, I+1])\n",
    "#                     edge1 = tmp_edges_1[a]\n",
    "#                     edge2 = tmp_edges_2[i]\n",
    "#                     c_e = np.dot(edge1, np.transpose(edge2))\n",
    "#                     c_e[np.isnan(c_e)] = 0\n",
    "#                     c_e[c_e == float('Inf')] = 0\n",
    "#                     Q[a,i] = sum(sum(c_e*m_Head))\n",
    "\n",
    "            # Node attribute\n",
    "            Q = Q + C_n \n",
    "#             print(Q)\n",
    "\n",
    "            # Update m_Head\n",
    "            m_Head = np.exp(beta*Q) \n",
    "            m_Head[-1, -1] = 0\n",
    "            \n",
    "#             print(m_Head)\n",
    "            \n",
    "            converge_C = False\n",
    "            I_C = 0\n",
    "            while (not converge_C) and I_C <= I_1: # Do C until C is converge or iteration exceeds\n",
    "                I_C += 1\n",
    "                old_C = m_Head\n",
    "                \n",
    "#                 print(m_Head[0])\n",
    "\n",
    "                # Begin alternative normalization. \n",
    "                # Do not consider the row or column of slacks\n",
    "                # by column\n",
    "                m_Head = normalize(m_Head, norm='l2',axis=0)*normalize(m_Head, norm='l2',axis=0)\n",
    "                # By row\n",
    "                m_Head = normalize(m_Head, norm='l2',axis=1)*normalize(m_Head, norm='l2',axis=1)\n",
    "                \n",
    "#                 print('I_C', m_Head[0])\n",
    "\n",
    "                # print(sum(m_Head))\n",
    "                # update converge_C\n",
    "                converge_C = abs(sum(sum(m_Head-old_C))) < e_C\n",
    "\n",
    "            # update converge_B\n",
    "            converge_B = abs(sum(sum(m_Head[:A,:I]-old_B[:A,:I]))) < e_B\n",
    "        # update beta\n",
    "        beta *= beta_r\n",
    "\n",
    "    match_matrix = heuristic(m_Head, A, I)\n",
    "    #match_matrix = m_Head\n",
    "    return match_matrix\n",
    "\n",
    "def heuristic(M, A, I):\n",
    "    '''\n",
    "    Make a soft assignment matrix to a permutation matrix. \n",
    "    Due to some rules.\n",
    "    We just set the maximum element in each column to 1 and \n",
    "    all others to 0.\n",
    "    This heuristic will always return a permutation matrix \n",
    "    from a row dominant doubly stochastic matrix.\n",
    "    '''\n",
    "    M = normalize(M, norm='l2',axis=1)*normalize(M, norm='l2',axis=1)\n",
    "    for i in range(A+1):\n",
    "        index = np.argmax(M[i,:]) # Get the maximum index of each row\n",
    "        M[i,:] = 0\n",
    "#         if index != I-1:\n",
    "#             M[:,index] = 0\n",
    "#         ###\n",
    "#         else:\n",
    "#             print(i, I-1)\n",
    "        M[i,index] = 1\n",
    "    M = M[:A,:I]\n",
    "    return M\n",
    "\n",
    "def match_difflen(pred, gt):\n",
    "    edges_pred = np.zeros((len(pred), len(pred)))\n",
    "    edges_gt = np.zeros((len(gt), len(gt)))\n",
    "\n",
    "    dist_matrix = cdist(pred, gt)\n",
    "    ARG1 = ARG(edges_pred, pred)\n",
    "    ARG2 = ARG(edges_gt, gt)\n",
    "    start_time = time.time()\n",
    "    C_n = pre_compute_compatibility( ARG1, ARG2, alpha=1, stochastic=0,node_binary=False)\n",
    "    # print(\"--- Calculate C_n,  %s hours ---\" % ((time.time() - start_time)/3600))\n",
    "    start_time = time.time()\n",
    "    match_matrix = graph_matching(C_n=C_n, ARG1 = ARG1, ARG2 = ARG2, \n",
    "                                beta_0=0.1, beta_f=100, beta_r=1.01, \n",
    "                                I_0=200, I_1=200, e_B=0.00005, e_C=0.00005\n",
    "                                )\n",
    "\n",
    "\n",
    "    g1, g2 = match_matrix.nonzero()\n",
    "    rslt = np.full([match_matrix.shape[0], 2], -1) # need to maintain record if one node is graph 1 is not matched\n",
    "\n",
    "    rslt[:, 0] = np.arange(0, match_matrix.shape[0])\n",
    "\n",
    "\n",
    "    for i in range(len(g1)):\n",
    "        rslt[g1[i], 1] = g2[i] \n",
    "    \n",
    "    # Find rows in array1 where the second column is -1\n",
    "    rows_to_match = np.where(rslt[:, 1] == -1)[0]\n",
    "    # For each of these rows, find the index of the minimum value in array2 and assign it\n",
    "    for row in rows_to_match:\n",
    "        rslt[row, 1] = np.argmin(dist_matrix[row])\n",
    "    return rslt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Calculate C_n,  9.773837195502386e-07 hours ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  4],\n",
       "       [ 1, -1],\n",
       "       [ 2,  2],\n",
       "       [ 3,  0],\n",
       "       [ 4,  3]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_rslt(pred,gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Calculate C_n,  1.6628371344672308e-06 hours ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 4],\n",
       "       [1, 1],\n",
       "       [2, 2],\n",
       "       [3, 0],\n",
       "       [4, 3]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_rslt(pred,gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 4],\n",
       "       [1, 1],\n",
       "       [2, 2],\n",
       "       [3, 0],\n",
       "       [4, 3]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_samelen(pred, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2765539 , 0.39952013],\n",
       "       [0.5683851 , 0.6435527 ],\n",
       "       [0.5973886 , 0.74988073],\n",
       "       [0.56094277, 0.74365056],\n",
       "       [0.38517028, 0.51225513]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5553    , 0.6967    ],\n",
       "       [0.57525   , 0.67340004],\n",
       "       [0.5968    , 0.6802    ],\n",
       "       [0.36099997, 0.49019998],\n",
       "       [0.2783    , 0.37449998]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/yunruili/anaconda_env/diffusion/lib/python3.8/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "graph_path_2dnmr = '/scratch0/yunruili/2dnmr_30k/graph_3d/'\n",
    "csv_file_2dnmr = '../nmr_smile_solvent_filtered2_3dgnn.csv'\n",
    "nmr_path_2dnmr = '/scratch0/yunruili/2dnmr_30k/csv_30k/'\n",
    "save_dir = '/scratch0/yunruili/2dnmr_30k/nmr_2dcsv_chmatched'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "dataset_2dnmr = graph_nmr_data_2d(csv_file_2dnmr, graph_path_2dnmr, nmr_path_2dnmr)\n",
    "dataloader_2dnmr = loader_2dnmr(dataset_2dnmr, shuffle=True, batch_size=1)\n",
    "\n",
    "list_files = []\n",
    "for i, data in enumerate(dataloader_2dnmr):\n",
    "    graph, cnmr, hnmr, filename = data\n",
    "    \n",
    "    c_nodes = (graph.x[:,0]==5).nonzero(as_tuple=True)[0]\n",
    "    h_nodes = (graph.x[:, 0] == 0).nonzero(as_tuple=True)[0] \n",
    "\n",
    "    c_shifts, h_shifts = model(graph)\n",
    "\n",
    "    ##### calculate the indices of C node connected to H\n",
    "    # Initialize a list to store C nodes connected to H\n",
    "    c_nodes_connected_to_h = []\n",
    "    # Check each C node for connection to any H node\n",
    "    for c_node in c_nodes:\n",
    "        # Get indices of edges involving the C node\n",
    "        edges_of_c = (graph.edge_index[0] == c_node) | (graph.edge_index[1] == c_node)\n",
    "\n",
    "        # Get all nodes that are connected to this C node\n",
    "        connected_nodes = torch.cat((graph.edge_index[0][edges_of_c], graph.edge_index[1][edges_of_c])).unique()\n",
    "\n",
    "        # Check if any of these connected nodes are H nodes\n",
    "        if any(node in h_nodes for node in connected_nodes):\n",
    "            c_nodes_connected_to_h.append(c_node.item())\n",
    "    \n",
    "    # Convert to a tensor\n",
    "    c_nodes_connected_to_h = torch.tensor(c_nodes_connected_to_h)\n",
    "\n",
    "    c_shifts = c_shifts.flatten().detach().numpy()\n",
    "    h_shifts = h_shifts.flatten().detach().numpy()\n",
    "    cnmr = cnmr.squeeze().detach().numpy()\n",
    "    hnmr = hnmr.squeeze().detach().numpy()\n",
    "\n",
    "    c_index = [i for i, x in enumerate(c_nodes) if x in c_nodes_connected_to_h]\n",
    "    c_shifts = c_shifts[c_index]\n",
    "\n",
    "    # print(filename)\n",
    "    # print(len(c_shifts), len(h_shifts))\n",
    "    # print(len(cnmr), len(hnmr))\n",
    "\n",
    "    # make c-h pairs for ground truth and prediction\n",
    "    hnmr_mean = np.mean(hnmr, axis=1)\n",
    "    pred = np.stack([c_shifts, h_shifts], axis=1)\n",
    "    gt = np.stack([cnmr, hnmr_mean], axis=1)\n",
    "\n",
    "    try:\n",
    "        if len(cnmr) == len(c_shifts):\n",
    "            rslt = match_samelen(pred, gt)\n",
    "        else:\n",
    "            rslt = match_difflen(pred, gt)\n",
    "\n",
    "        cnmr_expanded = np.array([[cnmr[i]] for i in rslt[:,1]])\n",
    "        hnmr_expanded = np.array([hnmr[i] for i in rslt[:, 1]])\n",
    "\n",
    "        # save as pickle\n",
    "        c_name = os.path.join(save_dir, '%s_c.pickle'%filename)\n",
    "        h_name = os.path.join(save_dir, '%s_h.pickle'%filename)\n",
    "        \n",
    "        # Writing to a pickle file\n",
    "        with open(c_name, 'wb') as file:\n",
    "            pickle.dump(cnmr_expanded, file)\n",
    "        with open(h_name, 'wb') as file:\n",
    "            pickle.dump(hnmr_expanded, file)\n",
    "    except Exception as e:\n",
    "        print(filename)\n",
    "        list_files.append(filename)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24416"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader_2dnmr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21300001, 0.21300001],\n",
       "       [0.21300001, 0.21300001],\n",
       "       [0.21300001, 0.21300001],\n",
       "       [0.10399999, 0.10399999],\n",
       "       [0.43      , 0.361     ],\n",
       "       [0.158     , 0.158     ],\n",
       "       [0.28399998, 0.26      ],\n",
       "       [0.191     , 0.191     ],\n",
       "       [0.21300001, 0.21300001],\n",
       "       [0.102     , 0.102     ],\n",
       "       [0.43      , 0.361     ],\n",
       "       [0.21300001, 0.21300001],\n",
       "       [0.372     , 0.372     ],\n",
       "       [0.41399997, 0.35      ],\n",
       "       [0.187     , 0.187     ],\n",
       "       [0.10399999, 0.10399999],\n",
       "       [0.21300001, 0.21300001],\n",
       "       [0.28399998, 0.26      ],\n",
       "       [0.148     , 0.148     ],\n",
       "       [0.41399997, 0.35      ],\n",
       "       [0.158     , 0.158     ],\n",
       "       [0.102     , 0.102     ],\n",
       "       [0.21300001, 0.21300001],\n",
       "       [0.28599998, 0.272     ],\n",
       "       [0.21300001, 0.21300001]], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hnmr_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16011375, 0.20083353, 0.17291945, 0.06110011, 0.38393286,\n",
       "       0.13929494, 0.06594548, 0.23464946, 0.17527401, 0.06670186,\n",
       "       0.36907583, 0.16188505, 0.06013245, 0.36011192, 0.1695453 ,\n",
       "       0.06381711, 0.2288571 , 0.11205356, 0.09719963, 0.36071712,\n",
       "       0.1470994 , 0.04043383, 0.22088648, 0.06940529, 0.13266073],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16011375, 0.22015314],\n",
       "       [0.20083353, 0.31865034],\n",
       "       [0.17291945, 0.26672673],\n",
       "       [0.06110011, 0.12806502],\n",
       "       [0.38393286, 0.40581673],\n",
       "       [0.13929494, 0.19710767],\n",
       "       [0.06594548, 0.24507643],\n",
       "       [0.23464946, 0.3259301 ],\n",
       "       [0.17527401, 0.23553558],\n",
       "       [0.06670186, 0.10809404],\n",
       "       [0.36907583, 0.40885496],\n",
       "       [0.16188505, 0.26829177],\n",
       "       [0.06013245, 0.4004858 ],\n",
       "       [0.36011192, 0.36829948],\n",
       "       [0.1695453 , 0.18305662],\n",
       "       [0.06381711, 0.13314608],\n",
       "       [0.2288571 , 0.30056593],\n",
       "       [0.11205356, 0.22901818],\n",
       "       [0.09719963, 0.13506696],\n",
       "       [0.36071712, 0.39051753],\n",
       "       [0.1470994 , 0.17446266],\n",
       "       [0.04043383, 0.098152  ],\n",
       "       [0.22088648, 0.3215965 ],\n",
       "       [0.06940529, 0.26445144],\n",
       "       [0.13266073, 0.20984401]], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37      , 0.38199997],\n",
       "       [0.176     , 0.21300001],\n",
       "       [0.2375    , 0.187     ],\n",
       "       [0.096     , 0.27899998],\n",
       "       [0.1145    , 0.148     ],\n",
       "       [0.0795    , 0.102     ],\n",
       "       [0.086     , 0.372     ],\n",
       "       [0.3725    , 0.3955    ],\n",
       "       [0.1765    , 0.21300001],\n",
       "       [0.2375    , 0.191     ],\n",
       "       [0.096     , 0.27199998],\n",
       "       [0.11149999, 0.158     ],\n",
       "       [0.079     , 0.10399999]], dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  8],\n",
       "       [ 1,  8],\n",
       "       [ 2,  1],\n",
       "       [ 3, 12],\n",
       "       [ 4,  7],\n",
       "       [ 5, 11],\n",
       "       [ 6, 10],\n",
       "       [ 7,  9],\n",
       "       [ 8,  8],\n",
       "       [ 9,  5],\n",
       "       [10,  7],\n",
       "       [11,  1],\n",
       "       [12,  6],\n",
       "       [13,  0],\n",
       "       [14,  2],\n",
       "       [15, 12],\n",
       "       [16,  8],\n",
       "       [17, 10],\n",
       "       [18,  4],\n",
       "       [19,  0],\n",
       "       [20, 11],\n",
       "       [21,  5],\n",
       "       [22,  8],\n",
       "       [23,  3],\n",
       "       [24,  1]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rslt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Calculate C_n,  2.545946174197727e-05 hours ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0, 12],\n",
       "       [ 1, 24],\n",
       "       [ 2, 16],\n",
       "       [ 3, 12],\n",
       "       [ 4, 12],\n",
       "       [ 5, 11],\n",
       "       [ 6,  4],\n",
       "       [ 7, 17],\n",
       "       [ 8,  5],\n",
       "       [ 9, 18],\n",
       "       [10, 23],\n",
       "       [11, 18],\n",
       "       [12,  6],\n",
       "       [13, 18],\n",
       "       [14,  1],\n",
       "       [15, 22],\n",
       "       [16,  9],\n",
       "       [17, 24],\n",
       "       [18, 16],\n",
       "       [19, 14],\n",
       "       [20, 20],\n",
       "       [21,  8],\n",
       "       [22,  2],\n",
       "       [23, 10],\n",
       "       [24, 19]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rslt = match_difflen(pred, gt)\n",
    "rslt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Calculate C_n,  2.4068421787685817e-05 hours ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0, 12],\n",
       "       [ 1, 24],\n",
       "       [ 2, 16],\n",
       "       [ 3, 12],\n",
       "       [ 4, 12],\n",
       "       [ 5, 11],\n",
       "       [ 6,  4],\n",
       "       [ 7, 17],\n",
       "       [ 8,  5],\n",
       "       [ 9, 18],\n",
       "       [10, 23],\n",
       "       [11, 18],\n",
       "       [12,  6],\n",
       "       [13, 18],\n",
       "       [14,  1],\n",
       "       [15, 22],\n",
       "       [16,  9],\n",
       "       [17, 24],\n",
       "       [18, 16],\n",
       "       [19, 14],\n",
       "       [20, 15],\n",
       "       [21,  8],\n",
       "       [22,  2],\n",
       "       [23, 10],\n",
       "       [24, 19]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rslt = match_difflen(pred, gt)\n",
    "rslt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
